---
title: "WQU Econometrics Week-3 Group Project Group 6-A"
output: html_notebook
---

## Group Member:  
- Dylan Thorne (dylan.thorne@gmail.com)  
- Mohammad Amimul Ihsan Aquil (amimul786@live.com)  
- Pariyat Limpitheeprakan (macchiato.me@gmail.com)  
- Trai Torsricharoen (traitorsricharoen@gmail.com)  
- YIU LEUNG CHENG (howard.yl.cheng.hk@gmail.com)  

# 3.1.1 Basic Statistics
## 3.1.1.1 Calculate in R
1.1. Average stock value  
1.2. Stock volatility  
1.3. Daily stock return  

```{r}

# 3.1.1.1 Calculate in R

jpmdata <- read.csv("JPM.csv",sep='|')
answer_average = mean(jpmdata[["Adj.Close"]])
rows = nrow(jpmdata)
returns <- log(jpmdata[2:rows,"Adj.Close"]/jpmdata[1:(rows-1),"Adj.Close"])
squared_returns <- returns*returns
answer_volatility = sqrt(sum(squared_returns)/(rows-1))
cat("1.1 Average stock value = ", answer_average, "\n")
cat("1.2 Stock volatility = ", answer_volatility, "\n")
cat("1.3 Daily returns\n")
print(returns)
```
## 3.1.1.2 Calculate in Excel:  
1.4. Average stock value  
1.5. Stock volatility  
1.6. Daily stock return  
1.7. Show JP Morgan stock price evolution using a scatter plot  
1.8. Add a trendline to the graph (trendline options â€“ linear)  

Please refer to [excel file here](./3.1.1 JPM.xlsx) 

# 3.1.2 Linear Regression
## 3.1.2.1 Implement a two-variable regression in R
```{r}

# 3.1.2.1 Implement a two-variable regression in R

sp500data <- read.csv("SP500.csv")
#data$x <- sp500data$Adj.Close
#data$y <- jpmdata$Adj.Close
#lm(data$y ~ data$x)
lm(jpmdata$Adj.Close ~ sp500data$Adj.Close)
```

## 3.1.2.1 Implement a two-variable regression in Excel using LINEST function and Analysis ToolPak

Please refer to complete calcuation in sheet [excel file here](./3.1.1 JPM.xlsx). 


# 3.1.3 Univariate Time Series Analysis
## 3.1.3.1 Forecast S&P/Case-Shiller U.S. National Home Price Index using an ARMA model.
Examine Given Data: A good stating point is to plot the series and visually examine it for any outliers, volatility, or irregularities.
```{r}
nhpidata <- read.csv("CSUSHPINSA.csv")

library(tidyverse)
library(stats)
library(tseries)
library(forecast)
library(zoo)
library(xts)
library(forecast)

nhpidata$DATE = as.Date(nhpidata$DATE)

ggplot(nhpidata, aes(DATE, CSUSHPINSA)) + geom_line() + scale_x_date('month')  + ylab("Monthly CSUSHPINSA") +
            xlab("")

#z = read.zoo(nhpidata)
#adf_test <- adf.test(z,alternative = 'stationary')
#print(adf_test)
```
As shown in the plot, the data exhibit trend line.

R provides a convenient method for removing time series outliers: `tsclean()` as part of its forecast package. tsclean() identifies and replaces outliers using series smoothing and decomposition. This method is also capable of inputing missing values in the series if there are any.Note that we are using the ts() command to create a time series object to pass to tsclean():

```{r}
CSUSHPINSA_ts = ts(nhpidata[, c('CSUSHPINSA')])

nhpidata$clean_CSUSHPINSA = tsclean(CSUSHPINSA_ts) # cleanse na or missing value

nhpidata <- nhpidata[,c(1,3)]

ggplot() +
  geom_line(data = nhpidata, aes(x = DATE, y = clean_CSUSHPINSA)) + ylab('Monthly Cleaned CSUSHPINSA')
```
The building blocks of a time series analysis are seasonality, trend, and cycle. Yet, not every series will have all three (or any) of these components, but if they are present, deconstructing the series can help us understand its behavior and prepare a foundation for building a forecasting model.

We can use the following R code to find out the components of this time series:
```{r}
count_ma = ts(na.omit(nhpidata$clean_CSUSHPINSA),start = c(1987,1),end = c(2019,6), frequency=12)
decomp = stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)
```
We now have a de-seasonalized series and can proceed to the next step.

### 3.1.3.1 Implement the Augmented Dickey-Fuller Test for checking the existence of a unitroot in Case-Shiller Index series
```{r}

adf.test(count_ma, alternative = "stationary")
```
The null hypothesis is NOT rejected as the p-value is greater than 0.05. So, the Case-Shiller Index series is non-stationary. This confirms the results of our visual inspection. Usually, non-stationary series can be corrected by a simple transformation such as differencing. Applying log and differencing into the series can help in removing its trend or cycles.

### 3.1.3.2 Implement an ARIMA(p,d,q) model. Determine p, d, q using Information Criterion or Box-Jenkins methodology. Comment result

```{r}
rows = nrow(nhpidata)
names(nhpidata) <- c('DATE','CSUSHPINSA')
#apply log to index to get log return
```

```{r}
returns <- log(nhpidata[2:rows,"CSUSHPINSA"]/nhpidata[1:(rows-1),"CSUSHPINSA"])
#plot log return against date
plot(returns)
#ADF test for stationarity of log return (suggesting nonstationarity)
adf.test(returns,alternative = "stationary")
```
Apply first difference:
```{r}
lengthOfReturns<-length(returns)
firstdiff<-returns[2:lengthOfReturns]-returns[1:lengthOfReturns-1]
adf.test(firstdiff)
```
P-value from the augmented Dickey-Fuller test  is less than 0.01 suggesting stationary.
Now, we can use `acf` and `pacf` to determine `p`, `d` and `q`
```{r}
pacf(firstdiff)
```
As the plot shown, there is strike in first 12 lags.

```{r}
acf(firstdiff)
```
As the plot above shown, acf pattern showing expoential decays with damped sine wave pattern. Therefore, it can be inferred that ARIMA(12,1,0) model fits the data well. Now we try with: 

```{r}
ARIMA_Model <- arima(window(firstdiff,1,lengthOfReturns-1), order=c(12,1,0), method = "ML")
summary(ARIMA_Model)
```
From the summary, we can see sigma^2 (variance) and aic are quite small, which is a good sign suggesting good fits to data. To check if the residual of ARIMA(12,1,0) model is pure white noise.
```{r}
pacf(ARIMA_Model$residuals)
acf(ARIMA_Model$residuals)
```

As shown in partial autocorrelation graph, we do not see significant strike except lag 16, 17. So, we can perform Box-Pierce test on these two lags.
```{r}
Box.test(ARIMA_Model$residuals, lag =16)
Box.test(ARIMA_Model$residuals, lag =17)
```
Their p-values are very high, so we fail to reject null hypothesis that errors are white noise. Therefore, the residuals are proved to be white noise.
### 3.1.3.3 Forecast the future evolution of Case-Shiller Index using the ARMA model. Test model using in-sample forecasts
```{r}
ARIMA_Model <- arima(window(returns,1,lengthOfReturns-73), order=c(12,1,0), method = "ML")
ARIMA_forecast <- predict(ARIMA_Model, n.ahead=72, se.fit=TRUE)
plot(window(returns, 1, lengthOfReturns-1))
lines(ARIMA_forecast$pred, col="blue")
lines(ARIMA_forecast$pred+1*ARIMA_forecast$se, col="cornflowerblue", lty="dashed")
lines(ARIMA_forecast$pred-1*ARIMA_forecast$se, col="cornflowerblue", lty="dashed")
```
To evaluate the model, we can start by examining ACF and PACF plots for model residuals. If model order parameters and structure are correctly specified, we would expect no significant autocorrelations present.
```{r}
tsdisplay(residuals(ARIMA_Model), main='(12,1,0) Model Residuals')
```
As we see in the graph, most of the test data are within one standard deviation from our prediction suggesting that ARIMA(12,1,0) is accurate.

Forecasting using a fitted model is straightforward in R. We can specify forecast horizon h periods ahead for predictions to be made, and use the fitted model to generate those predictions:
```{r}
futurVal <- forecast::forecast(ARIMA_Model,h=30, level=c(99.5))
plot(futurVal)
```
### 3.1.3.4 Suggest exogenous variables
It would be advisable to include these exogenous varibale into the modle: Federal Reserve Rate and Purchasing Manager Index. Motgage rate is closely relating to federal reserve rate, which will impact investors appetite on investing in property. Purchasing Manager Index is an indicator which suggests market sentiment and prevailing direction of economic trends in economic activities. Therefore, it can reflect investors' risk aversion in investing.
